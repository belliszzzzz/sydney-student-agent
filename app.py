import os
import glob
import json
from dataclasses import dataclass
from typing import List, Tuple
import dashscope


import numpy as np
import streamlit as st
from dotenv import load_dotenv

import faiss
from sentence_transformers import SentenceTransformer

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "last_module" not in st.session_state:
    st.session_state.last_module = "transport"

if "slot_state" not in st.session_state:
    st.session_state.slot_state = {}

# Optional LLM
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


load_dotenv()

INDEX_DIR = "embeddings"

MODULES = {
    "transport": "data/transport",
    "phone": "data/phone",
    "healthcare": "data/healthcare",
    "uni": "data/uni",
}

MODULE_SYSTEM_PROMPTS = {
    "transport": "ä½ æ˜¯æ‚‰å°¼å…¬å…±äº¤é€šä¸“å®¶ã€‚å›ç­”è¦ç»™å¯æ“ä½œæ­¥éª¤ã€å¡ç§/è´¹ç”¨/æ³¨æ„äº‹é¡¹ï¼Œå°½é‡ç”¨è¦ç‚¹åˆ—å‡ºã€‚",
    "phone": "ä½ æ˜¯æ¾³æ´²æ‰‹æœºå¡é¡¾é—®ã€‚å…ˆé—®æ¸…éœ€æ±‚ï¼ˆé¢„ç®—/æµé‡/è¦†ç›–/æ˜¯å¦åˆçº¦ï¼‰ï¼Œå†ç»™æ¨èæ–¹æ¡ˆå¯¹æ¯”ï¼šOptus/Telstra/Vodafone + prepaid/postpaidã€‚",
    "healthcare": "ä½ æ˜¯ç•™å­¦ç”ŸåŒ»ç–—ä¸OSHCé¡¾é—®ã€‚è§£é‡Šæµç¨‹ã€æŠ¥é”€æ­¥éª¤ã€GP/æ€¥è¯ŠåŒºåˆ«ã€ç´§æ€¥ç”µè¯ï¼Œç»™æ¸…æ™°æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ã€‚",
    "uni": "ä½ æ˜¯UNSWå­¦ç”Ÿäº‹åŠ¡é¡¾é—®ã€‚å›ç­”è¦åŒ…å«è§„åˆ™ç‚¹ã€å¯èƒ½åæœã€å»ºè®®åšæ³•ã€ä¸‹ä¸€æ­¥è”ç³»å¯¹è±¡ï¼ˆStudent Servicesç­‰ï¼‰ã€‚",
}

MODULE_REQUIREMENTS = {
    "transport": {
        "required": ["èµ·ç‚¹", "ç»ˆç‚¹"],
        "optional": ["æ˜¯å¦æºå¸¦è¡Œæ", "æ—¶é—´(å‡ ç‚¹å‡ºå‘)", "é¢„ç®—"],
        "question": "æˆ‘å¯ä»¥å¸®ä½ è§„åˆ’è·¯çº¿ï½å…ˆå‘Šè¯‰æˆ‘ï¼š1) ä»å“ªé‡Œå‡ºå‘ï¼Ÿ2) è¦å»å“ªé‡Œï¼Ÿï¼ˆå¯é€‰ï¼šå‡ºå‘æ—¶é—´/é¢„ç®—ï¼‰"
    },
    "phone": {
        "required": ["é¢„ç®—", "æµé‡éœ€æ±‚", "prepaid_or_postpaid"],
        "optional": ["æ˜¯å¦éœ€è¦å›½é™…é€šè¯", "æ˜¯å¦ç»å¸¸åè¿œåœ°åŒº", "æ˜¯å¦è¦eSIM"],
        "question": "æˆ‘å¯ä»¥å¸®ä½ é€‰æ‰‹æœºå¥—é¤ï½å…ˆç¡®è®¤ï¼š1) é¢„ç®—ï¼ˆæ¯æœˆä¸Šé™ï¼‰2) å¤§æ¦‚éœ€è¦å¤šå°‘GB/æœˆ 3) æƒ³è¦é¢„ä»˜prepaidè¿˜æ˜¯åˆçº¦postpaidï¼Ÿï¼ˆå¯é€‰ï¼šå›½é™…é€šè¯/eSIM/åè¿œåœ°åŒºï¼‰"
    },
    "healthcare": {
        "required": ["ä½ æ˜¯OSHCè¿˜æ˜¯Medicare", "éœ€æ±‚ç±»å‹(GP/æ€¥è¯Š/æŠ¥é”€)"],
        "optional": ["æ‰€åœ¨åŒº", "æ˜¯å¦ç´§æ€¥"],
        "question": "æˆ‘å¯ä»¥å¸®ä½ æ¢³ç†å°±åŒ»/æŠ¥é”€ï½å…ˆç¡®è®¤ï¼š1) ä½ æ˜¯OSHCå­¦ç”Ÿä¿é™©è¿˜æ˜¯Medicareï¼Ÿ2) ä½ è¦è§£å†³çš„æ˜¯çœ‹GP/æ€¥è¯Š/æŠ¥é”€å“ªä¸ªï¼Ÿ"
    },
    "uni": {
        "required": ["å­¦æ ¡", "é—®é¢˜ç±»å‹(æŒ‚ç§‘/ç‰¹æ®Šè€ƒè™‘/å­¦æœ¯è¯šä¿¡ç­‰)"],
        "optional": ["è¯¾ç¨‹ä»£ç ", "æˆªæ­¢æ—¥æœŸ"],
        "question": "æˆ‘å¯ä»¥æŒ‰å­¦æ ¡è§„åˆ™å¸®ä½ åˆ¤æ–­ï½å…ˆè¯´ï¼š1) ä½ å“ªä¸ªå­¦æ ¡ï¼ˆUNSWï¼Ÿï¼‰2) å±äºå“ªç±»é—®é¢˜ï¼ˆç‰¹æ®Šè€ƒè™‘/æŒ‚ç§‘/æŠ„è¢­/å‡ºå‹¤ç­‰ï¼‰ï¼Ÿ"
    },
}

MODULE_DISPLAY = {
    "transport": "ğŸš† äº¤é€šä¸“å®¶",
    "phone": "ğŸ“± æ‰‹æœºå¡ä¸“å®¶",
    "healthcare": "ğŸ¥ åŒ»ç–—ä¸“å®¶",
    "uni": "ğŸ“ å­¦ä¸šäº‹åŠ¡ä¸“å®¶"
}


LOCAL_EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"


@dataclass
class DocChunk:
    source: str
    text: str

def route_intent(query: str) -> str:
    q = query.lower()

    if any(k in q for k in ["sim", "telstra", "optus", "vodafone", "æ‰‹æœºå¡", "æµé‡", "å¥—é¤"]):
        return "phone"
    if any(k in q for k in ["oshc", "gp", "bulk billing", "000", "åŒ»ç–—", "çœ‹ç—…", "ä¿é™©", "æ€¥è¯Š"]):
        return "healthcare"
    if any(k in q for k in ["attendance", "special consideration", "plagiarism", "appeal", "å‡ºå‹¤", "å»¶æœŸ", "å­¦æœ¯è¯šä¿¡", "ç”³è¯‰"]):
        return "uni"
    return "transport"

INTENT_LABELS = {
    "transport": "public transport, opal card, train bus ferry, airport to city",
    "phone": "sim card, mobile plan, optus telstra vodafone prepaid postpaid",
    "healthcare": "oshc insurance, gp doctor, medicare bulk billing 000",
    "uni": "attendance, academic policy, special consideration, plagiarism"
}

SLOTS = {
  "phone": ["é¢„ç®—ä¸Šé™(æ¯æœˆ$)", "å¤§æ¦‚éœ€è¦å¤šå°‘GB", "prepaidè¿˜æ˜¯postpaid"],
  "transport": ["å‡ºå‘åœ°/ç›®çš„åœ°", "æ˜¯å¦éœ€è¦Opal", "æ˜¯å¦å‘¨æœ«/é«˜å³°"],
  "healthcare": ["æ˜¯å¦OSHC", "ç—‡çŠ¶ç´§æ€¥ç¨‹åº¦", "æ˜¯å¦éœ€è¦GP/æ€¥è¯Š"],
  "uni": ["å­¦æ ¡/è¯¾ç¨‹", "é—®é¢˜ç±»å‹(å‡ºå‹¤/å­¦æœ¯è¯šä¿¡/å»¶æœŸ)", "æˆªæ­¢æ—¶é—´"]
}

def route_intent_semantic(query: str, embedder):
    q_vec = embedder.encode([query])[0]
    best_k, best_score = None, -1.0

    for k, desc in INTENT_LABELS.items():
        d_vec = embedder.encode([desc])[0]
        score = float(np.dot(q_vec, d_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(d_vec) + 1e-9))
        if score > best_score:
            best_k, best_score = k, score

    return (best_k or "transport"), best_score

def is_opal_card_question(q: str) -> bool:
    ql = q.lower()
    return any(k in ql for k in ["opal", "opalå¡", "åŠå¡", "å……å€¼", "top up", "concession", "student fare", "ä¼˜æƒ ", "å­¦ç”Ÿç¥¨", "å¡ä¸¢äº†", "æŒ‚å¤±"])

def need_clarify(module: str, query: str) -> bool:
    # âœ… transport: Opal åŠå¡ç±»ä¸è¦æ±‚èµ·ç‚¹/ç»ˆç‚¹
    if module == "transport" and is_opal_card_question(query):
        return False

    req = MODULE_REQUIREMENTS.get(module, {})
    required = req.get("required", [])

    for r in required:
        if r in query:
            continue
        # phone å…œåº•é€»è¾‘ä¿ç•™...
        if module == "phone":
            ...
        return True
    return False




def split_markdown(text: str) -> List[str]:
    # ç®€å•åˆ‡å—ï¼šæŒ‰ç©ºè¡Œåˆ†æ®µï¼Œè¿‡æ»¤å¤ªçŸ­æ®µè½
    parts = [p.strip() for p in text.split("\n\n") if p.strip()]
    # å†è¿‡æ»¤ç‰¹åˆ«çŸ­çš„æ®µï¼ˆå°‘äº 40 å­—ç¬¦ï¼‰
    parts = [p for p in parts if len(p) >= 40]
    return parts

def load_docs(data_dir: str) -> List[DocChunk]:
    files = sorted(glob.glob(os.path.join(data_dir, "*.md")))
    chunks: List[DocChunk] = []
    for fp in files:
        with open(fp, "r", encoding="utf-8") as f:
            full = f.read().strip()
        if not full:
            continue

        parts = split_markdown(full)
        for i, p in enumerate(parts):
            chunks.append(
                DocChunk(
                    source=f"{os.path.basename(fp)}#chunk{i+1}",
                    text=f"(Source: {os.path.basename(fp)})\n{p}",
                )
            )
    return chunks



def embed_texts(model, texts):
    vecs = model.encode(texts, normalize_embeddings=True)
    return np.array(vecs, dtype="float32")


def build_or_load_index(module_name: str, chunks: List[DocChunk]):
    os.makedirs(INDEX_DIR, exist_ok=True)

    index_path = os.path.join(INDEX_DIR, f"{module_name}.faiss")
    meta_path = os.path.join(INDEX_DIR, f"{module_name}_meta.json")

    # load if exists
    if os.path.exists(index_path) and os.path.exists(meta_path):
        index = faiss.read_index(index_path)
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        embedder = SentenceTransformer(LOCAL_EMBED_MODEL, device="cpu")
        return index, meta, embedder

    # build
    embedder = SentenceTransformer(LOCAL_EMBED_MODEL)
    texts = [c.text for c in chunks]
    vectors = embedder.encode(texts, normalize_embeddings=True)
    vectors = np.array(vectors).astype("float32")

    dim = vectors.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(vectors)

    meta = [{"source": c.source, "text": c.text} for c in chunks]

    faiss.write_index(index, index_path)
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    return index, meta, embedder



def retrieve(query, index, meta, embedder, k=3):
    qv = embed_texts(embedder, [query])
    scores, ids = index.search(qv, k)

    results = []
    for score, idx in zip(scores[0], ids[0]):
        if idx == -1:
            continue
        item = meta[idx]
        results.append((float(score), item["source"], item["text"]))

    return results


def answer_with_llm(query, contexts, out_lang, module):
    provider = os.getenv("LLM_PROVIDER", "dashscope").strip().lower()

    if provider != "dashscope":
        return ""  # ä½ æš‚æ—¶åªç”¨ dashscope å°±å…ˆè¿™æ ·

    api_key = os.getenv("DASHSCOPE_API_KEY", "").strip()
    if not api_key:
        return "âŒ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼Œè¯·æ£€æŸ¥ .env æˆ–ç¯å¢ƒå˜é‡ã€‚"

    dashscope.api_key = api_key
    model = os.getenv("DASHSCOPE_MODEL", "qwen-turbo").strip()

    base_prompt = "ä½ æ˜¯æ‚‰å°¼ç•™å­¦ç”Ÿç”Ÿæ´»åŠ©æ‰‹ã€‚è¯·ç”¨æ¸…æ™°è¦ç‚¹å›ç­”ï¼Œå¿…è¦æ—¶åˆ†æ­¥éª¤ã€‚"
    system_prompt = MODULE_SYSTEM_PROMPTS.get(module, base_prompt)

    ctx_text = "\n\n".join([f"[{src}] {txt}" for _, src, txt in contexts]) if contexts else "ï¼ˆæ— æ£€ç´¢ç»“æœï¼‰"

    user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

å¯ç”¨èµ„æ–™ï¼ˆRAGæ£€ç´¢ï¼‰ï¼š
{ctx_text}

è¾“å‡ºè¯­è¨€ï¼š{out_lang}
è¯·åŸºäºèµ„æ–™å›ç­”ï¼›èµ„æ–™ä¸è¶³å°±æ˜ç¡®è¯´æ˜ï¼Œå¹¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ã€‚"""

    messages = [{"role": "system", "content": system_prompt}]

    # æ³¨å…¥æœ€è¿‘3è½®ï¼ˆ6æ¡ï¼‰ï¼Œä½†è¦ä¿è¯æ ¼å¼æ­£ç¡®
    hist = st.session_state.get("chat_history", [])
    if hist:
        messages += hist[-6:]

    messages.append({"role": "user", "content": user_prompt})

    resp = dashscope.Generation.call(
        model=model,
        messages=messages,
        result_format="message",
        temperature=0.2,
    )

    try:
        content = resp["output"]["choices"][0]["message"]["content"].strip()
        return content if content else "ï¼ˆLLM è¿”å›ç©ºå†…å®¹ï¼‰"
    except Exception:
        return f"âŒ DashScope è¿”å›å¼‚å¸¸ï¼š{resp}"






def fallback_answer(contexts, out_lang):
    if not contexts:
        return "å½“å‰çŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·å…ˆè¡¥å…… data/transport ä¸‹çš„æ–‡æ¡£ã€‚"

    top = contexts[0]  # (score, src, text)
    score, src, text = top

    if out_lang == "ä¸­æ–‡":
        return (
            "ã€æœ¬åœ°æ¨¡å¼ã€‘æœªé…ç½® API Keyï¼Œæ‰€ä»¥æ— æ³•è¿›è¡Œé«˜è´¨é‡ç¿»è¯‘/æ”¹å†™ã€‚æˆ‘å…ˆæŠŠæœ€ç›¸å…³çš„èµ„æ–™ç‰‡æ®µç»™ä½ ï¼š\n\n"
            f"æ¥æºï¼š{src}\n\n{text}"
        )
    else:
        return (
            "[Local mode] No API key configured, showing the most relevant retrieved text:\n\n"
            f"Source: {src}\n\n{text}"
        )

if "last_module" not in st.session_state:
    st.session_state.last_module = "transport"

current_module = st.session_state.last_module
# ---------------- UI ----------------

st.set_page_config(page_title="Sydney Student Agent", layout="wide")

st.title(f"Sydney International Student AI Assistant")



st.caption("RAG: FAISS + SentenceTransformers")

# =========================
# Chat å†å²æ˜¾ç¤ºåŒºåŸŸ
# =========================
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

embedder = SentenceTransformer(LOCAL_EMBED_MODEL)

indexes = {}
metas = {}

for module_name, data_dir in MODULES.items():
    docs_m = load_docs(data_dir)

    # âœ… å…³é”®ï¼šæŠŠæ¯ä¸ªæ¨¡å—è¯»åˆ°å¤šå°‘ chunk æ‰“å‡ºæ¥
    st.sidebar.write(f"ğŸ“š {module_name}: chunks={len(docs_m)} dir={data_dir}")

    if not docs_m:
        continue

    idx, meta_m, _ = build_or_load_index(module_name, docs_m)
    indexes[module_name] = idx
    metas[module_name] = meta_m

# âœ… å…³é”®ï¼šæŠŠæœ€ç»ˆå»ºå¥½çš„ç´¢å¼•æ¨¡å—åˆ—è¡¨æ‰“å°å‡ºæ¥
st.sidebar.write("âœ… indexes keys:", list(indexes.keys()))



with st.sidebar:
    st.json(st.session_state.slot_state)
    st.subheader("ç³»ç»ŸçŠ¶æ€")

    provider = os.getenv("LLM_PROVIDER", "dashscope").strip().lower()
    st.write("LLM_PROVIDER:", provider)

    if provider == "dashscope":
        has_key = bool(os.getenv("DASHSCOPE_API_KEY", "").strip())
        st.write("DASHSCOPE_API_KEY:", "âœ… å·²é…ç½®" if has_key else "âŒ æœªé…ç½®")
    else:
        has_key = bool(os.getenv("OPENAI_API_KEY", "").strip())
        st.write("OPENAI_API_KEY:", "âœ… å·²é…ç½®" if has_key else "âŒ æœªé…ç½®")

    st.subheader("è¾“å‡ºè¯­è¨€ / Output Language")
    out_lang = st.selectbox(
        "é€‰æ‹©å›ç­”è¯­è¨€",
        ["ä¸­æ–‡", "English", "FranÃ§ais", "æ—¥æœ¬èª", "í•œêµ­ì–´"],
        index=0
    )

    if st.button("é‡å»ºå‘é‡ç´¢å¼•"):
        # æ¸…ç†æ‰€æœ‰æ¨¡å—çš„ç´¢å¼•
        for module_name in MODULES.keys():
            index_path = os.path.join(INDEX_DIR, f"{module_name}.faiss")
            meta_path = os.path.join(INDEX_DIR, f"{module_name}_meta.json")
            if os.path.exists(index_path):
                os.remove(index_path)
            if os.path.exists(meta_path):
                os.remove(meta_path)
        
        st.success("ç´¢å¼•å·²é‡å»ºï¼Œè¯·åˆ·æ–°é¡µé¢")



query = st.chat_input("è¾“å…¥é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šæ€ä¹ˆåŠ Opal å¡ï¼ŸOptus å¥—é¤å“ªä¸ªå¥½ï¼Ÿï¼‰")

if query:
    query = query.strip()
    if not query:
        st.warning("è¯·è¾“å…¥é—®é¢˜")
        st.stop()

    # ===== 1) Router å†³ç­–ï¼ˆè¿™é‡Œä¸€å®šä¼šå®šä¹‰ moduleï¼‰=====
    module, score = route_intent_semantic(query, embedder)
    st.info(f"ğŸ§­ Router é€‰æ‹©æ¨¡å—: {module} (score={score:.3f})")

    AMBIGUOUS = ["ä¼˜æƒ ","æŠ˜æ‰£","å¤šå°‘é’±","ä»·æ ¼","æ€ä¹ˆå¼„","æ€ä¹ˆåŠ","æ€ä¹ˆåš","éœ€è¦ä»€ä¹ˆ","ææ–™","æµç¨‹"]
    last = st.session_state.get("last_module")

    if last and (len(query) <= 8 or any(k in query for k in AMBIGUOUS)):
        module = last
        st.info(f"â†©ï¸ æ¨¡ç³Šé—®é¢˜ï¼Œæ²¿ç”¨ä¸Šæ¬¡æ¨¡å—: {module}")

    if score < 0.20:
        module = route_intent(query)
        st.info(f"ğŸª è¯­ä¹‰åˆ†æ•°ä½ï¼Œå…³é”®è¯å…œåº•: {module}")

    # ===== 2) need_clarifyï¼ˆå¦‚æœè¦è¿½é—®ï¼Œç›´æ¥ stopï¼‰=====
    if need_clarify(module, query):
        st.session_state.last_module = module
        st.warning(MODULE_REQUIREMENTS[module]["question"])
        st.stop()

    # ===== 3) æ¨¡å—å¿…é¡»æœ‰ç´¢å¼• =====
    if module not in indexes:
        st.error(f"æ¨¡å— '{module}' æ²¡æœ‰ç´¢å¼•ã€‚å½“å‰å¯ç”¨æ¨¡å—: {list(indexes.keys())}")
        st.stop()

    # âœ… å…ˆè®°å½• last_moduleï¼ˆé¿å…åé¢ stop æ—¶ä¸¢å¤±ï¼‰
    st.session_state.last_module = module

    # ===== 4) Slot Fillingï¼ˆåªå¯¹ phone å¯ç”¨ï¼‰=====
    if "slot_state" not in st.session_state:
        st.session_state.slot_state = {}

    if module == "phone":
        if "phone" not in st.session_state.slot_state:
            st.session_state.slot_state["phone"] = {k: "" for k in SLOTS["phone"]}

        def update_slots_from_text(text):
            s = st.session_state.slot_state["phone"]
            t = text.lower()
            if any(x in t for x in ["$", "aud", "åˆ€", "ä»¥å†…", "ä»¥ä¸‹", "é¢„ç®—"]):
                s["é¢„ç®—ä¸Šé™(æ¯æœˆ$)"] = text
            if "gb" in t or "æµé‡" in text:
                s["å¤§æ¦‚éœ€è¦å¤šå°‘GB"] = text
            if any(x in t for x in ["prepaid", "postpaid", "é¢„ä»˜", "åˆçº¦"]):
                s["prepaidè¿˜æ˜¯postpaid"] = text

        update_slots_from_text(query)

        missing = [k for k, v in st.session_state.slot_state["phone"].items() if not v]
        if missing:
            ask = "æˆ‘éœ€è¦ä½ è¡¥å……ï¼š\n" + "\n".join([f"- {m}" for m in missing[:2]])
            st.session_state.chat_history.append({"role": "user", "content": query})
            st.session_state.chat_history.append({"role": "assistant", "content": ask})
            st.stop()

    # ===== 5) RAG æ£€ç´¢ =====
    contexts = retrieve(query, indexes[module], metas[module], embedder)

    st.subheader("ğŸ” RAG æ£€ç´¢ç»“æœ")
    for s, src, txt in contexts:
        with st.expander(f"{src} | score={s:.3f}"):
            st.write(txt)

    # ===== 6) ç”Ÿæˆç­”æ¡ˆ =====
    st.subheader("ğŸ¤– Agent å›ç­”")
    answer = answer_with_llm(query, contexts, out_lang, module)
    st.write(answer if answer else "ï¼ˆæœªç”Ÿæˆç­”æ¡ˆï¼‰")

    # ===== 7) å†™å…¥å†å²ï¼ˆä¸¥æ ¼ä¸¤æ¡ï¼‰=====
    st.session_state.chat_history.append({"role": "user", "content": query})
    st.session_state.chat_history.append({"role": "assistant", "content": answer if answer else "ï¼ˆæœªç”Ÿæˆç­”æ¡ˆï¼‰"})





